# -*- coding: utf-8 -*-
"""quering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EGaAwUpccdX1Jz2Qt4kkOADyczn632qf
"""

import numpy as np
import json
import sys

#from google.colab import drive
#drive.mount('/content/drive')

index1 = r"Re-cleaned-pr0f_data_cleaned/name_and_affiliation_index_full.json"
with open(index1) as f:
    name_and_affiliation_index = json.load(f)

index2 = r"Re-cleaned-pr0f_data_cleaned/topic_and_paper_index_full.json"
with open(index2) as f:
    topic_and_paper_index = json.load(f)

def default_ranking_metric(prof_data):
    h_ind = prof_data['h_ind']
    i_ind = prof_data['i_ind']
    h_ind5 = prof_data['h_ind5']
    i_ind5 = prof_data['i_ind5']
    cit = prof_data['cit']
    cit5 = prof_data['cit5']

    #the above parameters are adjusted as 
    # h_ind5, i_ind5 -> 70% 
    # cit5 -> 10%
    # h_ind, i_ind -> 17.5%
    # cit -> 2.5%

    parameters=[0.0875,0.0875,0.025,0.35,0.35,0.10]
    values = [h_ind, i_ind, cit, h_ind5, i_ind5, cit5]
    # these parameters can be made learnable for each user using machine learning.

    score = 0.0

    for i in range(len(values)):
        fraction = parameters[i]
        value = values[i]
        score += (fraction * value)

    return score

def merge_helper(list1, list2, req_dist = None):
    ptr1 = 0
    ptr2 = 0
    docs = []

    # Two pointers, increment the pointer that is behind the other.
    #basically, this function merges the doc value from the indexes
    # incase the req_dist is set,.. it is used to indicate the order of the words appear. So that the meaning wont change.
    while ptr1 < len(list1) and ptr2 < len(list2):
        if(list1[ptr1][0] == list2[ptr2][0]):
            # If req_dist is not set, then we do not need to check the second parameter (the position in the document).
            if req_dist == None:
                docs.append(list1[ptr1])
                ptr1 += 1
                ptr2 += 1
            # If req_dist is set, then we need to compare the distance between the positions in the documents with the req_dist.
            else:
                actual_dist = list2[ptr2][1] - list1[ptr1][1]
                if actual_dist == req_dist:
                    docs.append(list1[ptr1])
                    ptr1 += 1
                    ptr2 += 1
                # This means we need to decrease the actual_dist, or increment the first pointer
                elif actual_dist > req_dist:
                    ptr1 += 1
                # This means we need to increase the actual_dist, or increment the second pointer
                elif actual_dist < req_dist: 
                    ptr2 += 1
        elif list1[ptr1][0] < list2[ptr2][0]:
            ptr1 += 1
        elif list1[ptr1][0] > list2[ptr2][0]:
            ptr2 += 1

    return docs

def boolean_retrieval(parsed_query, use_topic_and_paper_index, AND = True):
    #parsed_query - user input (with or without stemming).
    # 2nd param - which should be considered
    # 3rd param - and or OR retreival method
    # in Boolean Retrieval, ignore the values of the key. That is, the second value in each tuple.
    # final_docs - list of all documents obtained as per parameters.
        

    documents_containing_word = dict()

    # Choose the index to be used
    if use_topic_and_paper_index:
        documents_containing_word = topic_and_paper_index
    else:
        documents_containing_word= name_and_affiliation_index

    # We store the postings list of all the words found in the index.
    docs_list = []
    for word in parsed_query:
        if word in documents_containing_word:
            docs_list.append(documents_containing_word[word])
        else:
            docs_list.append([])    
    #print(docs_list)
    final_docs = []

    if AND:

        # Find the lengths of the postings_list if AND is used. This is because sort by the lengths is an optimisation for AND. It is not so for OR.
        len_list = [len(docs_with_word) for docs_with_word in docs_list]

        # Handle the case when no match.
        if len(docs_list) == 0:
            return []

        # For efficiency, sort lists by their sizes, then merge.
        sort_by_len = np.argsort(len_list)
        new_docs_list = [None] * len(docs_list)
        for i in range(len(docs_list)):
            new_docs_list[i] = docs_list[sort_by_len[i]]
        
        #print(new_docs_list)
        #remove all empty lists
        i=0
        while len(new_docs_list[i])==0:
            new_docs_list.pop(i)
        #print(new_docs_list)

        # At the ith iteration, store the list of all documents for the first (i + 1) words.
        final_list = new_docs_list[0]
        if len(new_docs_list) != 1:
            for i in range(1, len(new_docs_list)):
                final_list = merge_helper(final_list, new_docs_list[i])
        #print(final_docs)
        
        # Return only the documents where these words occur, not the positions.
        for i in range(len(final_list)):
            if i != 0:
                if final_list[i][0] != final_docs[-1]:
                    final_docs.append(final_list[i][0])
            else:
                final_docs.append(final_list[0][0]) 

    
    # OR operation
    else:

        # Couple of dictionaries defined as follows-
        # Doc : Total number of words found in that document. This acts as a tiebreaker between documents having the same "score".
        doc_freq = dict() 
        # Doc : Set of all query words that appear in it. The length of this set is the "score" of each document.
        doc_words = dict() 
        for i, postings_list in zip(range(len(docs_list)), docs_list):
            for doc in postings_list:
                doc_id = doc[0]
                if doc_id not in doc_freq:
                    doc_words[doc_id] = set()
                    doc_freq[doc_id] = 0
                doc_words[doc_id].add(i)
                doc_freq[doc_id] += 1

        # Sort documents in decreasing order of "score" as defined above.
        # If two documents have the same score, then the document which has a higher doc_freq, that is, a higher total number of matches will be returned first.
        for doc_id in doc_freq.keys():
            final_docs.append((doc_id, (len(doc_words[doc_id]), doc_freq[doc_id])))
        final_docs = sorted(final_docs, key = lambda x : x[1], reverse = True)

        # We return only the document ids, but return them in order.
        for i in range(len(final_docs)):
            final_docs[i] = final_docs[i][0]
            
    return final_docs

#boolean_retrieval("machin machin",True,True)

#get_tokenized_words("machine learning",False)

def phrase_retrieval(parsed_phrase, use_topic_and_paper_index):
    # Obtain the postings lists of the words in the phrase
    # Sort them in the order of their lengths
    # Perform a merge wherever two documents are the same, and their positions in the given document have the required distance
    # Perform a "merge" for every pair of words in the parsed_phrase. However this time their positions in the given document have the required distance(as much as in the parsed_phrase).
         

    documents_containing_word = dict()

    if use_topic_and_paper_index:        
        documents_containing_word = topic_and_paper_index
    else:    
        documents_containing_word = name_and_affiliation_index

    # Repeat words in a phrase not handled.
    ctr = 0
    word_pos = dict()
    for word in parsed_phrase:
        word_pos[word] = ctr
        ctr += 1
    
    # We store the postings list of all the words found in the index.
    docs_list = []
    for word in parsed_phrase:        
        if word in documents_containing_word:
            docs_list.append(documents_containing_word[word])
        else:
            docs_list.append([])    
    # Find the lengths of the postings_list of all documents.
    len_list = [len(docs_with_word) for docs_with_word in docs_list]

    # Handle the case when no match.
    if len(docs_list) == 0:
        return []

    # For efficiency, sort lists by their sizes, then merge.
    sort_by_len = np.argsort(len_list)
    new_docs_list = [None] * len(docs_list)
    for i in range(len(docs_list)):
        new_docs_list[i] = docs_list[sort_by_len[i]]

    final_list = new_docs_list[0]
    # At the ith iteration, store the list of all documents for the first (i + 1) words.
    if len(new_docs_list) != 1:
        for i in range(1, len(new_docs_list)):
            distance = sort_by_len[i] - sort_by_len[0]
            final_list = merge_helper(final_list, new_docs_list[i], distance)

    # Return only the documents where these words occur, not the positions.
    final_docs = []
    for i in range(len(final_list)):
        if i != 0:
            if final_list[i][0] != final_docs[-1]:
                final_docs.append(final_list[i][0])
        else:
            final_docs.append(final_list[0][0]) 
            
    return final_docs