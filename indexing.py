# -*- coding: utf-8 -*-
"""Indexing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nzI-oufAy-4tCi2IHqVFJQ7qEeS6XOzJ
"""

import csv
import json
import pandas as pd
import sys
import re
from tqdm import tqdm

'''a="Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization. Tokenization also helps to substitute sensitive data elements with non-sensitive data elements. Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve the above-stated purpose."
b = get_tokenized_words(a,True)
print(type(b))
print(b)'''

import nltk
nltk.download('punkt')

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Project

#pip install import-ipynb
import import_ipynb

import helperMethods

# Commented out IPython magic to ensure Python compatibility.
# %run 'helperMethods.ipynb' import *

prof_topic_and_paper_index = dict()
prof_name_and_affiliation_index = dict()

#remove_stop_words_and_perform_stemming-rswpm
def build_index_helper(prof_id, string1, string2, rswpm, index):    
  #concatenate name and affiliation
    string_for_index_building = string1+" "+string2
    words = get_tokenized_words(string_for_index_building,rswpm)        
    for pos_ind, key in zip(range(len(words)), words):
        if key in index:            
            index[key].append((prof_id, pos_ind))
        else:
            index[key] = [(prof_id, pos_ind)]

def build_index(prof_id, name, affiliation, topics_list, papers_title_list):      
    # Name and Affiliation Index
    build_index_helper(prof_id, name, affiliation, False, prof_name_and_affiliation_index)

    # Topics and Paper Title Index
    topics_string = " ".join(topics_list)
    papers_string = " ".join(papers_title_list)
    build_index_helper(prof_id, topics_string, papers_string, True, prof_topic_and_paper_index)

def make_list_citations(initial_string):
    try:
        return list(map(int,initial_string.lstrip('[').rstrip(']').split(', ')))
    except:
        return []

file_count = 26
filepath=r"Re-cleaned-pr0f_data_cleaned"
for file_index in range(file_count):   

    try:
        input_file = pd.read_csv(filepath+'/pr0f_data-'+chr(ord('a')+file_index)+'-cleaned.csv',header=None,encoding='utf8')
    except:
        print("Error in opening input file.")
        sys.exit(0)
    input_file.drop(index=0,inplace=True)
    number_of_professors = len(input_file)
    
    for prof_index in tqdm(range(number_of_professors)):    
        scholar_id = input_file.iloc[prof_index][0]
        name = check_for_nan(input_file.iloc[prof_index][1])
        affiliation = check_for_nan(input_file.iloc[prof_index][3])
        email = check_for_nan(input_file.iloc[prof_index][4])
        homepage = check_for_nan(input_file.iloc[prof_index][5])
        topics_list = make_list(input_file.iloc[prof_index][6])
        cit = int(input_file.iloc[prof_index][7])
        h_ind = int(input_file.iloc[prof_index][8])
        i_ind = int(input_file.iloc[prof_index][9])
        cit5 = int(input_file.iloc[prof_index][10])
        h_ind5 = int(input_file.iloc[prof_index][11])
        i_ind5 = int(input_file.iloc[prof_index][12])    
        cit_list = make_list_citations(input_file.iloc[prof_index][13])
        image_url = check_for_nan(input_file.iloc[prof_index][14])
        papers_url_list = make_list(input_file.iloc[prof_index][15])
        papers_title_list = make_list(input_file.iloc[prof_index][16])          
        prof_id = get_id(file_index, prof_index)

        build_index(prof_id, name, affiliation, topics_list, papers_title_list) 

    print("File "+str(file_index)+" Completed.")        

print("Number of keywords in name and affiliation index - "+str(len(prof_name_and_affiliation_index)))
print("Number of keywords in topic and paper index - "+str(len(prof_topic_and_paper_index)))

with open(r'Re-cleaned-pr0f_data_cleaned\name_and_affiliation_index_full.json', 'w+',encoding='utf8') as outfile:
    json.dump(prof_name_and_affiliation_index, outfile)

with open(r'Re-cleaned-pr0f_data_cleaned\topic_and_paper_index_full.json', 'w+',encoding='utf8') as outfile:
    json.dump(prof_topic_and_paper_index, outfile)



input_file.columns
input_file.drop(index=0,inplace=True)
input_file.head()

import nltk
nltk.download('punkt')